{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM22OW5IZEk2r9kTflcS3Bs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED5GAOfVygu6","executionInfo":{"status":"ok","timestamp":1717294132082,"user_tz":-480,"elapsed":23865,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"edae5806-2974-4582-c0ba-3b974e5cf241"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import numpy as np\n","drive.mount('/content/drive')\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Load the Policy Metadata file\n","csv_file = 'directory of policy meta data file'\n","\n","# Create a dictionary to map ID to state and action year\n","id_to_state_and_year = dict(zip(metadata['ID'].astype(str), zip(metadata['State / Jurisdiction'], metadata['Action_Year'])))\n","\n","# Function to match txt file names with state and action year\n","def get_state_and_year_for_file(file_name, id_to_state_and_year):\n","    # Handle cases with letters in file names (e.g., 11a, 11b)\n","    base_file_name = ''.join([char for char in file_name if char.isdigit()])\n","    return id_to_state_and_year.get(base_file_name)\n","\n","text_files_directory = \"Directory containing the text files\"\n","# Create a DataFrame to store file names, their states, and action years\n","file_state_year_data = []\n","for file_name in os.listdir(text_files_directory):\n","    if file_name.endswith('.txt'):\n","        state_year_info = get_state_and_year_for_file(file_name.split('.')[0], id_to_state_and_year)\n","        if state_year_info:\n","            state, action_year = state_year_info\n","            file_state_year_data.append({'File Name': file_name, 'State': state, 'Action Year': action_year})\n","\n","# Convert to DataFrame\n","file_state_df = pd.DataFrame(file_state_year_data)\n","file_state_df['Action Year']=file_state_df['Action Year'].astype(\"int\")\n","\n","print(file_state_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbOFSl39yi12","executionInfo":{"status":"ok","timestamp":1717294133173,"user_tz":-480,"elapsed":1092,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"ea913246-c488-4025-d4ad-f29757c4943a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    File Name          State  Action Year\n","0       3.txt         Alaska         2016\n","1       4.txt         Alaska         2018\n","2       7.txt       Arkansas         2017\n","3       8.txt       Arkansas         2015\n","4      10.txt        Arizona         2021\n","..        ...            ...          ...\n","151   147.txt      Wisconsin            0\n","152   149.txt  West Virginia            0\n","153    62.txt       Michigan         1987\n","154    40.txt        Indiana            0\n","155    87.txt         Nevada            0\n","\n","[156 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["# Preprocessing\n","import nltk\n","import re\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import string\n","nltk.download('stopwords')\n","class TextProcessor:\n","    def __init__(self, input_corpus: list):\n","        self.processed_corpus = input_corpus\n","\n","    def remove_digits_and_punctuation(self):\n","        regex_pattern = re.compile(r'[0-9]+|[^\\w\\s]')\n","        self.processed_corpus = [regex_pattern.sub(\" \", row) for row in self.processed_corpus]\n","\n","    def to_lowercase(self):\n","        self.processed_corpus = [row.lower() for row in self.processed_corpus]\n","\n","    def remove_stop_words(self):\n","        stops = set(stopwords.words('english')) - {\n","            \"shan't\", \"couldn't\", \"against\", \"shouldn't\", \"can't\",\n","            \"needn't\", \"should've\", \"not\", \"mustn't\", \"will\"\n","        }\n","        stops.update(string.ascii_lowercase)\n","        stops.update(['ii', 'iii', 'iv'])\n","        self.processed_corpus = [\n","            \" \".join([token for token in row.split() if token not in stops])\n","            for row in self.processed_corpus\n","        ]\n","\n","    def remove_common_words(self):\n","        counter = Counter(\" \".join(self.processed_corpus).split())\n","        most_common = set(word for word, count in counter.most_common(10))\n","        self.processed_corpus = [\n","            \" \".join(token for token in row.split() if token not in most_common)\n","            for row in self.processed_corpus\n","        ]\n","\n","    def process(self):\n","        self.remove_digits_and_punctuation()\n","        self.to_lowercase()\n","        self.remove_stop_words()\n","        self.remove_common_words()\n","        return self.processed_corpus\n","\n","def read_text_files(directory):\n","    file_contents = {}\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".txt\"):\n","            file_path = os.path.join(directory, filename)\n","            try:\n","                with open(file_path, 'r', encoding='utf-8') as file:\n","                    file_contents[filename] = file.read()\n","            except UnicodeDecodeError:\n","                try:\n","                    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n","                        file_contents[filename] = file.read()\n","                except Exception as e:\n","                    print(f\"Failed to read {filename}: {e}\")\n","    return file_contents\n","# Load and preprocess the text files\n","file_directory = \"Directory containing the text files\"\n","file_contents = read_text_files(file_directory)\n","text_processor = TextProcessor(list(file_contents.values()))\n","preprocessed_texts = text_processor.process()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IL7ONIqayksF","executionInfo":{"status":"ok","timestamp":1717252668866,"user_tz":-480,"elapsed":7270,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"d99ff1ed-af91-4f72-eb88-a98390b16058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["file_to_state = dict(zip(file_state_df['File Name'], file_state_df['State']))\n","file_to_year_map = dict(zip(file_state_df['File Name'], file_state_df['Action Year']))\n","file_to_state_map = dict(zip(file_state_df['File Name'], file_state_df['State']))"],"metadata":{"id":"8urRh8Hbym8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HikuXDCgyozo","executionInfo":{"status":"ok","timestamp":1717252702107,"user_tz":-480,"elapsed":33261,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"de13a9ae-2f26-40b2-eaef-f32d4867d390"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UCQMdIXyqdY","executionInfo":{"status":"ok","timestamp":1717252714327,"user_tz":-480,"elapsed":12226,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"d08a924f-9c15-4b3c-b39e-500ef41c00bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["!pip install fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xvn4Lhi2yslV","executionInfo":{"status":"ok","timestamp":1717252736503,"user_tz":-480,"elapsed":22177,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"8a989a9d-140b-44e3-c577-290c35d46d7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"]}]},{"cell_type":"code","source":["import fasttext\n","import fasttext.util\n","fasttext.util.download_model('en', if_exists='ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uQg5HcXzyufy","executionInfo":{"status":"ok","timestamp":1717252736503,"user_tz":-480,"elapsed":3,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"1b3cac2d-341a-4ecb-87da-1de936b1fcaa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cc.en.300.bin'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import zipfile\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_zip_path = '/content/drive/MyDrive/Policy_comparison/glove.6B.zip'\n","glove_folder = '/content/glove.6B'\n","with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(glove_folder)\n","glove_input_file = os.path.join(glove_folder, 'glove.6B.300d.txt')\n","word2vec_output_file = os.path.join(glove_folder, 'glove.6B.300d.word2vec.txt')\n","glove2word2vec(glove_input_file, word2vec_output_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKWKrQA3ywCs","executionInfo":{"status":"ok","timestamp":1717253054136,"user_tz":-480,"elapsed":317635,"user":{"displayName":"Selena Ding","userId":"08752327328823649622"}},"outputId":"7e9b52d4-3106-4aea-9323-e51075e937f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-80b4c3d88f92>:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n","  glove2word2vec(glove_input_file, word2vec_output_file)\n"]},{"output_type":"execute_result","data":{"text/plain":["(400001, 300)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from collections import defaultdict\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import PCA\n","from sentence_transformers import SentenceTransformer\n","from transformers import BertTokenizer, BertModel\n","import torch\n","import fasttext\n","from gensim.models import KeyedVectors\n","from collections import defaultdict\n","import numpy as np\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from sklearn.decomposition import PCA\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import NMF\n","from sklearn.metrics.pairwise import cosine_similarity\n","from transformers import BertTokenizer, BertModel\n","import torch\n","from gensim.models import LdaMulticore\n","from gensim.models import KeyedVectors\n","from gensim.corpora import Dictionary\n","from nltk.tokenize import word_tokenize\n","import fasttext\n","\n","# Define functions for each model\n","def get_tfidf_nmf_embeddings(texts, n_components=10, ngram_range=(1, 1)):\n","    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.95, min_df=2, lowercase=False)\n","    tfidf = vectorizer.fit_transform(texts)\n","    nmf = NMF(n_components=n_components, random_state=42)\n","    nmf_features = nmf.fit_transform(tfidf)\n","    return nmf_features\n","\n","def get_sentence_transformer_embeddings(texts):\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","    embeddings = model.encode(texts, convert_to_tensor=False)\n","    pca = PCA(n_components=10)\n","    reduced_embeddings = pca.fit_transform(embeddings)\n","    return reduced_embeddings\n","\n","def get_bert_embeddings(texts):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n","    outputs = model(**encoded_input)\n","    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n","    return embeddings\n","\n","def get_fasttext_embeddings(texts):\n","    model = fasttext.load_model('cc.en.300.bin')\n","    return np.array([model.get_sentence_vector(text) for text in texts])\n","\n","def get_glove_embeddings(texts):\n","    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","    embeddings = [np.mean([glove_model[word] for word in text.split() if word in glove_model], axis=0) for text in texts if text.split()]\n","    pca = PCA(n_components=10)\n","    reduced_embeddings = pca.fit_transform(embeddings)\n","    return reduced_embeddings\n","\n","# Function to find similar policies\n","def find_similar_policies_A(single_policy, all_policies, threshold = 0.5):\n","    models = {\n","        'TFIDF-NMF-1': lambda texts: get_tfidf_nmf_embeddings(texts, 10, (1, 1)),\n","        'TFIDF-NMF-2': lambda texts: get_tfidf_nmf_embeddings(texts, 10, (1, 2)),\n","        'SentenceTransformer': get_sentence_transformer_embeddings,\n","        'BERT': get_bert_embeddings,\n","        'FastText': get_fasttext_embeddings,\n","        'GloVe': get_glove_embeddings\n","    }\n","\n","    all_policies_filtered = [p for p in all_policies if p != single_policy]\n","    results = defaultdict(list)\n","\n","    for name, model_func in models.items():\n","        embeddings = model_func([single_policy] + all_policies_filtered)\n","        single_embedding = embeddings[0]\n","        other_embeddings = embeddings[1:]\n","        similarities = cosine_similarity([single_embedding], other_embeddings)[0]\n","\n","        for idx, similarity_score in enumerate(similarities):\n","            if similarity_score > threshold:\n","                results[(all_policies_filtered[idx], similarity_score)].append(name)\n","\n","    option_a = {processed_to_original[content]: list(models.keys()) for content, models in model_votes.items() if len(models) >= 2}\n","    option_c = {processed_to_original[content]: model_names for content, model_names in top_pairs_per_model.items() if len(set(model_names)) >= 2}\n","\n","    return option_a, option_c\n","\n","original_texts = list(file_contents.values())  # List of original texts\n","option_a, option_c = find_similar_policies(single_policy, preprocessed_texts, original_texts, threshold=0.8, top_n=10)\n","\n"],"metadata":{"id":"ogY81rKsyxrX"},"execution_count":null,"outputs":[]}]}